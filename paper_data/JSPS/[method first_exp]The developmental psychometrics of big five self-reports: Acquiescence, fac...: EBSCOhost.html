<span class="medium-bold"><h3><a data-auto="ep_link" href="#toc" onclick="FocusElement('toc');" id="hd_toc_psp-94-4-718-ID0ECCAA" title="Method">Method</a></h3></span><br /><a id="psp-94-4-718-ID0EDCCAA"> </a><h4>Sample</h4><p class="body-paragraph" data-auto="body_paragraph">Participants were 230,047 volunteers who provided <strong data-auto="strong_text">personality</strong> and demographics data over the World Wide Web. All respondents were between the ages of 10 and 20 (<em>M</em> = 16.6 years) and were residents of either the United States (90% of the sample) or Canada (10%). In preliminary data screening, a small proportion (2.<strong data-auto="strong_text">5</strong>%) of cases was eliminated from the final sample because multiple questions were left unanswered or because the same response was repeated over and over (cf. <a href="#c14">Costa &amp; McCrae, 1992</a>; <a href="#c28">Gosling, Vazire, Srivastava, &amp; John, 2004</a>).<a id="b-fn3"> </a><sup><a href="#fn3" /></sup> This exclusion rate is similar to what we have observed with undergraduate samples (ages 18 to 23 years) recruited from a research participant pool; in such samples, exclusion rates have typically varied between 2% and <strong data-auto="strong_text">5</strong>%. Even after the elimination of potentially suspect cases, sample sizes for each specific year of age were large: 1,754 for age 10; 3,590 for age 11; 6,508 for age 12; 14,310 for age 13; 21,152 for age 14; 27,138 for age 15; 31,272 for age 16; 32,483 for age 17; 36,397 for age 18; 29,317 for age 19; and 26,126 for age 20.<a id="b-fn4"> </a><sup><a href="#fn4" /></sup></p><p class="body-paragraph" data-auto="body_paragraph">Compared with non-Internet samples recruited for <strong data-auto="strong_text">personality</strong> and <strong data-auto="strong_text">social</strong> <strong data-auto="strong_text">psychology</strong> studies published in the <em><strong data-auto="strong_text">Journal</strong> of <strong data-auto="strong_text">Personality</strong> and <strong data-auto="strong_text">Social</strong> <strong data-auto="strong_text">Psychology</strong></em> (<a href="#c28">Gosling et al., 2004</a>), the present sample was split more evenly between male and female participants and was at least as diverse in terms of ethnicity. Overall, 60% of respondents were female and 40% were male. Respondents reported their ethnicity as one of seven categories: 12,540 (<strong data-auto="strong_text">5</strong>.<strong data-auto="strong_text">5</strong>%) were Asian; 7,684 (3.3%) were Black; 9,895 (4.3%) were Latino; 972 (0.4%) were Middle Eastern; 1,468 (0.6%) were Native American; 178,761 (77.7%) were White; 11,865 (<strong data-auto="strong_text">5</strong>.2%) reported their ethnicity as Other; and 6,862 (3.0%) did not <strong data-auto="strong_text">report</strong> their ethnicity.</p><p class="body-paragraph" data-auto="body_paragraph">The sample was also diverse in terms of socioeconomic status. A question about <strong data-auto="strong_text">social</strong> class was added during the survey period, and answers to this question were available for most of the sample. Of these respondents, 11,866 (9.2%) described their parents' <strong data-auto="strong_text">social</strong> class as working class; 13,586 (10.<strong data-auto="strong_text">5</strong>%) as lower-middle class; 55,758 (43.2%) as middle class; 43,198 (33.4%) as upper-middle class; and 4,729 (3.7%) as upper class.</p><p class="body-paragraph" data-auto="body_paragraph">Because of the large sample size, all significance tests of individual-level (as opposed to group-level) variables were evaluated at the conservative α = .001 level. Tests of group-level variables were evaluated at the conventional α = .05 level.</p><a id="psp-94-4-718-ID0ECCCAA"> </a><h4>Procedure and Data Collection</h4><p class="body-paragraph" data-auto="body_paragraph">The data analyzed here were collected by Jeff Potter via his Web site (<a href="http://www.outofservice.com" target="_blank">www.outofservice.com</a>) and made available for scientific analysis in exchange for authorship. The outofservice.com Web site offers its visitors free feedback on several surveys and <strong data-auto="strong_text">personality</strong> measures. Potential respondents could reach this Web site in a number of ways, including search engines, unsolicited links from other Web sites, and informal channels, such as e-mail and online discussion forums.</p><p class="body-paragraph" data-auto="body_paragraph">Many children and adolescents may have difficulty appreciating the risks and costs of participating in a research procedure, so special care must be taken to protect the welfare of minor participants in scientific research. For most research involving minors, this includes obtaining the informed consent of parents for their child's participation. Initially, Jeff Potter—a Web entrepreneur who is not a psychologist or associated with an academic institution—obtained a portion of the present data without seeking review from an institutional review board (IRB). When the potential significance of this emerging data set became apparent, Oliver P. John and Samuel D. Gosling obtained separate IRB approvals to (a) analyze the then-existing data archive and (b) collect new data. The present research, including a waiver of parental consent, was approved by two IRBs (a) because the research procedure—anonymously answering questions about global <strong data-auto="strong_text">personality</strong> characteristics and receiving general and innocuous feedback—involves no more than minimal risk to the participants, (b) because the waiver of consent would not adversely affect the rights or welfare of the participants, and (c) because the research could not practicably be carried out without the waiver (<a href="#c68">United States Department of Health and Human Services, 2005</a>). More generally, we encourage researchers interested in collecting questionnaire data from minors, whether over the Internet or by other means, to carefully consider the welfare of their participants when selecting the questions to be asked and any feedback to be given.</p><a id="psp-94-4-718-ID0EBCCAA"> </a><h4>Measurement</h4><p class="body-paragraph" data-auto="body_paragraph"><strong>The BFI</strong></p><p class="body-paragraph" data-auto="body_paragraph">The BFI (<a href="#c38">John et al., 1991</a>; <a href="#c39">John et al., in press</a>; <a href="#c40">John &amp; Srivastava, 1999</a>) was used to obtain <strong data-auto="strong_text">personality</strong> <strong data-auto="strong_text">self</strong>-<strong data-auto="strong_text">reports</strong>. The 44 BFI items are short, easy-to-understand phrases that assess <strong data-auto="strong_text">personality</strong> traits central to each of the <strong data-auto="strong_text">Big</strong> <strong data-auto="strong_text">Five</strong> domains. An example item from the Extraversion scale is “Is outgoing, sociable”; a reverse-keyed example item from the Neuroticism scale is “Is emotionally stable, not easily upset.” Each item was rated on a scale ranging from 1 (<em>strongly disagree</em>) to <strong data-auto="strong_text">5</strong> (<em>strongly agree</em>). The brevity of the BFI, and its fifth-grade reading level (<a href="#c8">Benet-Martínez &amp; John, 1998</a>), make it well suited to a survey of young respondents that expects each participant to devote only a limited amount of time. In adult samples, the BFI scales have shown high internal consistency, retest reliability, and clear factor structure, as well as strong convergence with longer <strong data-auto="strong_text">Big</strong> <strong data-auto="strong_text">Five</strong> measures (e.g., <a href="#c8">Benet-Martínez &amp; John, 1998</a>; <a href="#c39">John et al., in press</a>; <a href="#c40">John &amp; Srivastava, 1999</a>) and substantial agreement between <strong data-auto="strong_text">self</strong>- and peer-<strong data-auto="strong_text">reports</strong> (e.g., <a href="#c15">DeYoung, 2006</a>; <a href="#c57">Rammstedt &amp; John, 2007</a>).</p><p class="body-paragraph" data-auto="body_paragraph"><strong>Measuring individual differences in acquiescent response style</strong></p><p class="body-paragraph" data-auto="body_paragraph">Individual differences in acquiescent responding were indexed with acquiescence scores (within-person response means) computed from a set of 16 pairs of BFI items with opposite implications for <strong data-auto="strong_text">personality</strong> (e.g., “Is talkative” vs. “Tends to be quiet”; see the <a href="#A">Appendix</a> for a complete list of these pairs as well as scoring procedures). These pairs were selected by Christopher J. Soto and Oliver P. John on the basis of item content and interitem correlations. An acquiescence score was computed for each participant across the 32 items from these pairs (rather than from the full set of 44 BFI items) because, in the full set, direction of item keying is confounded with <strong data-auto="strong_text">personality</strong> content. Specifically, all of the BFI scales have more true-keyed items than false-keyed items, although the ratio of true- to false-keyed items varies across scales. This confound is not present in the set of 16 opposite item pairs, which equates, for each <strong data-auto="strong_text">Big</strong> <strong data-auto="strong_text">Five</strong> domain, the number of true-keyed and false-keyed items.</p><p class="body-paragraph" data-auto="body_paragraph"><strong>Ipsatized data: controlling for response style</strong></p><p class="body-paragraph" data-auto="body_paragraph">Ipsatization (or within-person standardization) is the procedure of transforming data so that each participant's set of item responses has the same mean and standard deviation (e.g., <a href="#c49">McCrae et al., 2001</a>; <a href="#c66">Ten Berge, 1999</a>). Many researchers routinely ipsatize their data to control for individual differences in acquiescent and extreme responding (e.g., <a href="#c7">Ashton et al., 2004</a>; <a href="#c25">Goldberg, 1990</a>, <a href="#c26">1992</a>). Typically, ipsatization is accomplished by subtracting each participant's mean response (across all items) from each of their individual item responses, then dividing these differences by the standard deviation of that person's responses. The resulting set of transformed responses has a mean of 0 and a standard deviation of 1 for each participant.<a id="b-fn5"> </a><sup><a href="#fn5" /></sup> To avoid confounding response style with <strong data-auto="strong_text">personality</strong> content (as described in the previous paragraph), within-person response means and standard deviations here were computed across the 32 items from the set of 16 opposite-item pairs; these values were then used to ipsatize the full set of 44 items, as detailed in the <a href="#A">Appendix</a>.</p><p class="body-paragraph" data-auto="body_paragraph"><strong>Assessing the comprehension difficulty of the BFI items: item understandability and trait-word familiarity</strong></p><p class="body-paragraph" data-auto="body_paragraph">Two approaches were used to assess comprehension difficulties posed by the BFI items. First, following <a href="#c6">Angleitner et al. (1986)</a>, 13 <strong data-auto="strong_text">psychology</strong> graduate students reviewed the BFI items and then judged how difficult each item would be for a typical 10-year-old to understand. As in <a href="#c6">Angleitner et al. (1986)</a>, <strong data-auto="strong_text">psychology</strong> students were used as judges because of their familiarity with <strong data-auto="strong_text">personality</strong> measures and their general psychological mindedness. These judges were asked to consider both the semantic and syntactic features of the items; an easy-to-understand item was defined as one that is clearly written and uses only basic vocabulary (i.e., words that almost all fourth graders/10-year-olds would know), whereas a difficult item was defined as one that uses more advanced vocabulary or syntax. Each judge rated each BFI item on a scale ranging from 1 (<em>very easy to understand</em>) to 7 (<em>very difficult to understand</em>). Across the set of 44 items, the alpha reliability for the mean of the 13 judges was .93.</p><p class="body-paragraph" data-auto="body_paragraph">A second approach to assessing comprehension difficulty focused exclusively on a semantic aspect of the BFI items, namely the unfamiliarity of the trait concepts being assessed: Items that include unfamiliar trait terms are presumably more difficult for respondents to comprehend. Demonstrating that the familiarity of trait terms can be assessed reliably, <a href="#c60">Saucier (1997)</a> collected familiarity ratings for 1,248 person-descriptive adjectives from a sample of 112 adult community residents and undergraduate <strong data-auto="strong_text">psychology</strong> students, using a scale ranging from 0 (<em>I don't know the meaning of the word</em>) to 9 (<em>I know the meaning of the word and think it is used extremely often to describe people</em>). The alpha reliability of these judgments exceeded .90.</p><p class="body-paragraph" data-auto="body_paragraph">The BFI item phrases were derived from trait adjectives that experts had rated as uniquely prototypical of each <strong data-auto="strong_text">Big</strong> <strong data-auto="strong_text">Five</strong> domain (see <a href="#c36">John, 1990</a>), and so we were able to index the unfamiliarity of each item by matching its <strong data-auto="strong_text">personality</strong> descriptors to <a href="#c60">Saucier's (1997)</a> list of ratings. We reverse-keyed Saucier's original ratings so that high scores would indicate unfamiliarity—that is, items that were more unfamiliar and thus more difficult to comprehend. For items that included more than one descriptor (e.g., “Is outgoing, sociable”), we indexed overall unfamiliarity as the mean of all adjectives for which ratings from <a href="#c60">Saucier (1997)</a> were available. For four BFI items (e.g., “Tends to find fault with others”), the corresponding trait adjectives (e.g., faultfinding) were not included on the <a href="#c60">Saucier (1997)</a> list, and so these four items could not be used in the present analyses of unfamiliarity.</p><p class="body-paragraph" data-auto="body_paragraph">Examples of BFI items that were identified by both understandability and unfamiliarity ratings as particularly difficult are “Is ingenious, a deep thinker” (difficulty of understanding = <strong data-auto="strong_text">5</strong>.15, unfamiliarity = 3.72); “Can be cold and aloof” (difficulty of understanding = <strong data-auto="strong_text">5</strong>.15, unfamiliarity = 2.58); and “Likes to reflect, play with ideas (difficulty of understanding = <strong data-auto="strong_text">5</strong>.08, unfamiliarity = 3.76). These items clearly include more difficult language than do three items identified as particularly easy: “Is talkative” (difficulty of understanding = 1.23, unfamiliarity = 1.48); “Is sometimes rude to others” (difficulty of understanding = 1.62, unfamiliarity = 0.76); and “Tends to be quiet” (difficulty of understanding = 1.77, unfamiliarity = 0.92).</p><p class="body-paragraph" data-auto="body_paragraph">How should these two approaches to defining comprehension difficulty be related? Note that the difficulty-of-understanding ratings took into account both the syntax and semantics of the full item text, whereas the unfamiliarity scores were based only on the individual trait terms. Moreover, the difficulty-of-understanding judges focused on the language skills of typical 10-year-olds, whereas the adults and undergraduates who judged familiarity in <a href="#c60">Saucier's (1997)</a> research were asked to consider only their own knowledge of the <strong data-auto="strong_text">personality</strong> terms. Despite these differences, difficulty of understanding and unfamiliarity were strongly intercorrelated across the 40 BFI items for which the unfamiliarity index was available (<em>r</em> = .48, <em>p</em> &lt; .05).</p><p class="body-paragraph" data-auto="body_paragraph">We also tested whether these two sets of ratings were sufficiently sensitive to children's verbal abilities. Three elementary school teachers particularly familiar with the vocabulary and reading skills of 10- and 11-year-old children rated how difficult it would be for a typical 10-year-old (fourth grader) to understand each of the BFI items. Across the 44 items, the means of the teacher ratings correlated very strongly with the means of the graduate student ratings (<em>r</em> = .82, <em>p</em> &lt; .05). These teacher ratings also correlated substantially with the unfamiliarity scores derived from <a href="#c60">Saucier (1997</a>; <em>r</em> = .39, <em>p</em> &lt; .05).<a id="b-fn6"> </a><sup><a href="#fn6" /></sup></p><a id="psp-94-4-718-ID0EACCAA"> </a><h4>Gender Differences</h4><p class="body-paragraph" data-auto="body_paragraph">In preliminary analyses, we examined differences between the characteristics of responses made by male and female participants. At each age, within-domain coherence and between-domain differentiation for males and females differed by no more than two hundredths of a correlation point. More importantly, age trends in coherence and differentiation were not substantively different for male and female participants. Therefore, specific tests of gender differences are not reported here. Instead, the confounding of age effects with gender effects was avoided by analyzing male and female participants separately. In the interest of clarity, all figures plot age-group values as the unweighted average of the male and female values.</p><a id="psp-94-4-718-ID0EBCAA"> </a>